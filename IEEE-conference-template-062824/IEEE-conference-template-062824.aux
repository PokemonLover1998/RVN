\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\citation{pacheco2016iot,naveed2024towards}
\citation{morales2024dsl,chen2023automated}
\citation{ayoughi2024enhancing,weber2024model}
\citation{kaufmann2020deep,madry2017towards}
\citation{morgulis2019fooling}
\citation{weng2018evaluating}
\citation{silva2020opportunities,amini2021robust,zhou2022adversarial,wang2024survey}
\citation{bai2021recent,zhao2024attack}
\citation{weng2018evaluating}
\citation{pacheco2016iot,sathiyanarayanan2018smart}
\citation{hassanalian2017classifications,saunders2024autonomous}
\citation{nubert2020safe,abu2022deep,kohler2020computationally}
\citation{liu2023modeling,kohler2020computationally}
\citation{ivanov2021verisig,huang2022polar,sha2021synthesizing}
\citation{xue2020inner,xue2021reach,zhang2023reachability}
\citation{xue2020inner}
\citation{dai2021lyapunov,wu2023neural,yanglyapunov}
\citation{chakraborty2011nonlinear,chen2014stability}
\citation{korda2014controller,mauroy2016global}
\citation{wu2023neural,yanglyapunov}
\citation{dai2021lyapunov}
\citation{lyapunov1992general}
\citation{yanglyapunov}
\citation{weng2018evaluating,zhao2024attack}
\citation{weng2018evaluating}
\citation{wang2024survey}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\citation{vidal1988implementing,nubert2020safe}
\citation{luenberger1971introduction,xue2020inner,meyn2022control}
\citation{luenberger1971introduction,crary1990techniques,chen2015disturbance}
\citation{luenberger1971introduction}
\citation{yanglyapunov}
\citation{luenberger1971introduction}
\citation{yanglyapunov}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {I-}0a}Organaization.}{2}{paragraph.1.0.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries}{2}{section.2}\protected@file@percent }
\newlabel{sec:pre}{{II}{2}{Preliminaries}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Neural Network Controlled Systems (\textsc  {NNCS}\xspace  )}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:plant1}{{1a}{2}{Neural Network Controlled Systems (\nncs )}{equation.2.1a}{}}
\newlabel{eq:plant2}{{1b}{2}{Neural Network Controlled Systems (\nncs )}{equation.2.1b}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}1}State feedback control}{2}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq:state_feedback_control}{{2}{2}{State feedback control}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In state feedback secnario, the \textsc  {NNCS}\xspace  has a plant and a controller implemented by a neural network $\pi $. In output feedback secnario, the \textsc  {NNCS}\xspace  has a plant, a controller neural network $\pi $, and an observer neural network $\phi _{obs}$.}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:two_feedback_secnarios}{{1}{2}{In state feedback secnario, the \nncs has a plant and a controller implemented by a neural network $\pi $. In output feedback secnario, the \nncs has a plant, a controller neural network $\pi $, and an observer neural network $\phi _{obs}$}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}2}Output feedback control}{2}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{eq:output_feedback_state}{{3a}{2}{Output feedback control}{equation.2.3a}{}}
\newlabel{eq:output_feedback_control}{{3b}{2}{Output feedback control}{equation.2.3b}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}3}Union formulation}{2}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq:state_feedback_dynamic}{{4}{2}{Union formulation}{equation.2.4}{}}
\citation{xue2020inner,xue2021reach}
\citation{xue2020inner}
\citation{xue2020inner}
\citation{yanglyapunov}
\citation{dai2021lyapunov,wu2023neural,yanglyapunov}
\citation{lyapunov1992general}
\newlabel{eq:output_feedback_dynamic}{{5}{3}{Union formulation}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Reach-Avoid set}{3}{subsection.2.2}\protected@file@percent }
\newlabel{eq:RA 1}{{6}{3}{Reach-Avoid set}{equation.2.6}{}}
\newlabel{eq:RA 2}{{7a}{3}{Reach-Avoid set}{equation.2.7a}{}}
\newlabel{eq:RA 3}{{7b}{3}{Reach-Avoid set}{equation.2.7b}{}}
\newlabel{eq:RA 4}{{8}{3}{Reach-Avoid set}{equation.2.8}{}}
\newlabel{eq:lapunov optimization}{{9a}{3}{Reach-Avoid set}{equation.2.9a}{}}
\newlabel{eq:RA condition1}{{9b}{3}{Reach-Avoid set}{equation.2.9b}{}}
\newlabel{eq:RA condition2}{{9c}{3}{Reach-Avoid set}{equation.2.9c}{}}
\newlabel{eq:RA condition3}{{9d}{3}{Reach-Avoid set}{equation.2.9d}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Region of attraction}{3}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The $\mathcal  {B}$ is the region of interest, $\mathcal  {A}$ is a non-covex region of attraction which is hard to be detected, and $\mathcal  {S}=\{\boldsymbol  {x}_{t}\in \mathcal  {B}| V(\boldsymbol  {\xi }_{t}) < \rho \}$ is the convex inner-approximation of $\mathcal  {A}$, $V(\boldsymbol  {\xi }):\mathbb  {R}^{n_{\boldsymbol  {\xi }}} \to \mathbb  {R}$ is a Lyapunov function and $\boldsymbol  {\xi }_{t} = [\boldsymbol  {x}_{t}, \boldsymbol  {e}_{t}]^{T}$. }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:roa}{{2}{3}{The $\calB $ is the region of interest, $\calA $ is a non-covex region of attraction which is hard to be detected, and $\calS =\{\myvec {x}_{t}\in \calB | V(\myvec {\xi }_{t}) < \rho \}$ is the convex inner-approximation of $\calA $, $V(\myvec {\xi }):\bbR ^{n_{\myvec {\xi }}} \to \bbR $ is a Lyapunov function and $\myvec {\xi }_{t} = [\myvec {x}_{t}, \myvec {e}_{t}]^{T}$}{figure.caption.2}{}}
\newlabel{eq:lapunov optimization}{{10a}{3}{Region of attraction}{equation.2.10a}{}}
\newlabel{eq:lyapunov condition1}{{10b}{3}{Region of attraction}{equation.2.10b}{}}
\newlabel{eq:lyapunov condition2}{{10c}{3}{Region of attraction}{equation.2.10c}{}}
\newlabel{eq:lyapunov condition3}{{10d}{3}{Region of attraction}{equation.2.10d}{}}
\newlabel{eq:lapunov function}{{11}{3}{Region of attraction}{equation.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Robustness evaluation for \textsc  {NNCS}\xspace  }{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{III}{3}{Robustness evaluation for \nncs }{section.3}{}}
\citation{chakraborty2011nonlinear,chen2014stability}
\citation{korda2014controller,mauroy2016global}
\citation{wu2023neural,yanglyapunov}
\citation{weng2018evaluating}
\newlabel{def:minimum distortion}{{3}{4}{Robustness evaluation for \nncs }{definition.3}{}}
\newlabel{lemma:CLEVER}{{1}{4}{Robustness evaluation for \nncs }{lemma.1}{}}
\newlabel{theorem:nncs Lipschitz continuity}{{1}{4}{Robustness evaluation for \nncs }{theorem.1}{}}
\newlabel{proof:nncs Lipschitz continuity}{{1}{4}{Robustness evaluation for \nncs }{proof.1}{}}
\newlabel{eq:nncs Lipschitz continuity 1}{{13}{4}{Robustness evaluation for \nncs }{equation.3.13}{}}
\newlabel{eq:nncs Lipschitz continuity 1.1}{{14}{4}{Robustness evaluation for \nncs }{equation.3.14}{}}
\newlabel{eq:nncs Lipschitz continuity 1.2}{{15}{4}{Robustness evaluation for \nncs }{equation.3.15}{}}
\newlabel{eq:nncs Lipschitz continuity 2}{{16}{4}{Robustness evaluation for \nncs }{equation.3.16}{}}
\newlabel{eq:nncs Lipschitz continuity 3}{{17}{4}{Robustness evaluation for \nncs }{equation.3.17}{}}
\newlabel{eq:nncs Lipschitz continuity 4}{{19}{4}{Robustness evaluation for \nncs }{equation.3.19}{}}
\newlabel{eq:nncs Lipschitz continuity 5}{{20}{4}{Robustness evaluation for \nncs }{equation.3.20}{}}
\newlabel{eq:nncs Lipschitz continuity 6}{{21a}{4}{Robustness evaluation for \nncs }{equation.3.21a}{}}
\newlabel{eq:nncs Lipschitz continuity 7}{{21b}{4}{Robustness evaluation for \nncs }{equation.3.21b}{}}
\citation{weng2018evaluating}
\citation{wood1996estimation}
\citation{weng2018evaluating}
\newlabel{eq:nncs Lipschitz continuity 8}{{22}{5}{Robustness evaluation for \nncs }{equation.3.22}{}}
\newlabel{eq:nncs Lipschitz continuity 9}{{23a}{5}{Robustness evaluation for \nncs }{equation.3.23a}{}}
\newlabel{eq:nncs Lipschitz continuity 10}{{23b}{5}{Robustness evaluation for \nncs }{equation.3.23b}{}}
\newlabel{eq:nncs Lipschitz continuity 11}{{24}{5}{Robustness evaluation for \nncs }{equation.3.24}{}}
\newlabel{theorem:Robustness evaluation for NNCS}{{2}{5}{Robustness evaluation for \nncs }{theorem.2}{}}
\newlabel{eq:Robustness evaluation for NNCS 1}{{25}{5}{Robustness evaluation for \nncs }{equation.3.25}{}}
\newlabel{proof:Robustness evaluation for NNCS}{{2}{5}{Robustness evaluation for \nncs }{proof.2}{}}
\newlabel{eq:Robustness evaluation for NNCS 2}{{26}{5}{Robustness evaluation for \nncs }{equation.3.26}{}}
\newlabel{eq:Robustness evaluation for NNCS 3}{{27}{5}{Robustness evaluation for \nncs }{equation.3.27}{}}
\newlabel{eq:Robustness evaluation for NNCS 4}{{28}{5}{Robustness evaluation for \nncs }{equation.3.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}\textsc  {REN}\xspace  evaluation via different distributions}{5}{section.4}\protected@file@percent }
\newlabel{sec:algorithm}{{IV}{5}{\ren evaluation via different distributions}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Estimate $L_{q}$ via two different distributions}{5}{subsection.4.1}\protected@file@percent }
\newlabel{sec:esimate in two distributions}{{\mbox  {IV-A}}{5}{Estimate $L_{q}$ via two different distributions}{subsection.4.1}{}}
\newlabel{lemma:differentiable}{{2}{5}{Estimate $L_{q}$ via two different distributions}{lemma.2}{}}
\newlabel{proof:differentiable}{{3}{5}{Estimate $L_{q}$ via two different distributions}{proof.3}{}}
\newlabel{eq:differentiable 1}{{29a}{5}{Estimate $L_{q}$ via two different distributions}{equation.4.29a}{}}
\newlabel{eq:differentiable 2}{{29b}{5}{Estimate $L_{q}$ via two different distributions}{equation.4.29b}{}}
\newlabel{eq:differentiable 2.5}{{30}{5}{Estimate $L_{q}$ via two different distributions}{equation.4.30}{}}
\citation{weng2018evaluating}
\citation{lyapunov1992general}
\citation{lyapunov1992general}
\citation{ponce2007continuity}
\citation{de1963subcompactness}
\citation{kechris1990classification}
\citation{burr1942cumulative}
\citation{smith1990extreme}
\citation{smith1990extreme}
\newlabel{eq:indicator function}{{31}{6}{Estimate $L_{q}$ via two different distributions}{equation.4.31}{}}
\newlabel{eq:differentiable 3}{{32a}{6}{Estimate $L_{q}$ via two different distributions}{equation.4.32a}{}}
\newlabel{eq:differentiable 4}{{32b}{6}{Estimate $L_{q}$ via two different distributions}{equation.4.32b}{}}
\newlabel{lemma:differentiable continuity}{{3}{6}{Estimate $L_{q}$ via two different distributions}{lemma.3}{}}
\newlabel{theorem:different distributions}{{3}{6}{Estimate $L_{q}$ via two different distributions}{theorem.3}{}}
\newlabel{proof:different distributions}{{4}{6}{Estimate $L_{q}$ via two different distributions}{proof.4}{}}
\newlabel{eq:monotonicity 1}{{33}{6}{Estimate $L_{q}$ via two different distributions}{equation.4.33}{}}
\newlabel{lemma:mle}{{4}{6}{Estimate $L_{q}$ via two different distributions}{lemma.4}{}}
\citation{yanglyapunov}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Algorithm for \textsc  {REN}\xspace  }{7}{subsection.4.2}\protected@file@percent }
\newlabel{algorithm}{{\mbox  {IV-B}}{7}{Algorithm for \ren }{subsection.4.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm of \textsc  {REN}\xspace  }}{7}{algorithm.1}\protected@file@percent }
\newlabel{algorithm REN}{{1}{7}{Algorithm of \ren }{algorithm.1}{}}
\newlabel{code:initialize}{{3}{7}{Algorithm of \ren }{ALC@unique.3}{}}
\newlabel{code:pure sample}{{5}{7}{Algorithm of \ren }{ALC@unique.5}{}}
\newlabel{code:distortion S1}{{8}{7}{Algorithm of \ren }{ALC@unique.8}{}}
\newlabel{code:distortion S2}{{10}{7}{Algorithm of \ren }{ALC@unique.10}{}}
\newlabel{code:perturbed sample}{{15}{7}{Algorithm of \ren }{ALC@unique.15}{}}
\newlabel{code:sample calculation}{{17}{7}{Algorithm of \ren }{ALC@unique.17}{}}
\newlabel{code:maximum batch}{{19}{7}{Algorithm of \ren }{ALC@unique.19}{}}
\newlabel{code:Reverse Weibull}{{22}{7}{Algorithm of \ren }{ALC@unique.22}{}}
\newlabel{code:one-point}{{24}{7}{Algorithm of \ren }{ALC@unique.24}{}}
\newlabel{code:robustness evaluation}{{26}{7}{Algorithm of \ren }{ALC@unique.26}{}}
\citation{snider2009automatic}
\citation{mori1976control}
\citation{bouabdallah2007full}
\citation{brix2024fifth}
\citation{yanglyapunov}
\citation{yanglyapunov}
\citation{yanglyapunov}
\citation{madry2017towards}
\citation{madry2017towards,du2021fast}
\citation{yanglyapunov}
\citation{yanglyapunov}
\citation{madry2017towards}
\citation{wang2021beta}
\citation{madry2017towards}
\citation{wang2021beta}
\citation{massey1951kolmogorov}
\citation{yanglyapunov}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{8}{section.5}\protected@file@percent }
\newlabel{sec:exp}{{V}{8}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Experiment Configurations}{8}{subsection.5.1}\protected@file@percent }
\newlabel{exam:configuration}{{\mbox  {V-A}}{8}{Experiment Configurations}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {V-A}0a}Networks and tasks}{8}{paragraph.5.1.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {V-A}0b}Baselines}{8}{paragraph.5.1.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {V-A}0c}Machine and Software}{8}{paragraph.5.1.0.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Models used in the experiments. All the models are trained through PGD~\cite  {madry2017towards} and $\alpha \beta $-CROWN\xspace  ~\cite  {wang2021beta} based on Lyapunov conditions. The Neurons and Layers imply the number of neurons and layers in one time interval. The terms "-state" or "-output" in Model denotes that the scenario of the task involves state feedback or output feedback. The 'small' means that the \textsc  {NNCS}\xspace  is trained with a small torque. }}{8}{table.caption.3}\protected@file@percent }
\newlabel{table:NN_table}{{I}{8}{Models used in the experiments. All the models are trained through PGD~\cite {madry2017towards} and \abcrown ~\cite {wang2021beta} based on Lyapunov conditions. The Neurons and Layers imply the number of neurons and layers in one time interval. The terms "-state" or "-output" in Model denotes that the scenario of the task involves state feedback or output feedback. The 'small' means that the \nncs is trained with a small torque}{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Validation of distinct distributions}{8}{subsection.5.2}\protected@file@percent }
\newlabel{exam:validation}{{\mbox  {V-B}}{8}{Validation of distinct distributions}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The cross Lipschitz constant sampled in the pathtracking tasks, varying the batch size ($N_b$), the number of samples per batch ($N_s$), while maintaining a constant time step ($t = 10$). The values of $N_b$, $N_s$, and p-values (pVal) from K-S test are presented atop each subfigure. The blue parts represent the empirical distribution of the Lipschitz constants, whereas the red curve denotes the theoretical Reverse Weibull distribution. }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:experiment1.1}{{3}{8}{The cross Lipschitz constant sampled in the pathtracking tasks, varying the batch size ($N_b$), the number of samples per batch ($N_s$), while maintaining a constant time step ($t = 10$). The values of $N_b$, $N_s$, and p-values (pVal) from K-S test are presented atop each subfigure. The blue parts represent the empirical distribution of the Lipschitz constants, whereas the red curve denotes the theoretical Reverse Weibull distribution}{figure.caption.4}{}}
\citation{jay1981gender}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The cross Lipschitz constant sampled in the pathtracking tasks, varying the time step $t$, while maintaining a constant $N_b\times N_s$. The values of $t$, the p-values (pVal) from K-S test, and robustness evaluation of \textsc  {REN}\xspace  (delta) are presented atop each subfigure. }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:experiment1.2}{{4}{9}{The cross Lipschitz constant sampled in the pathtracking tasks, varying the time step $t$, while maintaining a constant $N_b\times N_s$. The values of $t$, the p-values (pVal) from K-S test, and robustness evaluation of \ren (delta) are presented atop each subfigure}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}Comparison between robustness evaluation from \textsc  {REN}\xspace  and barrier functions}{9}{subsection.5.3}\protected@file@percent }
\newlabel{exam:efficiency}{{\mbox  {V-C}}{9}{Comparison between robustness evaluation from \ren and barrier functions}{subsection.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Both \textsc  {REN}\xspace  and $L$ robustness evaluations are derived from the same set of 100 samples. Red points represent the $L$ robustness evaluation, while blue points denote the \textsc  {REN}\xspace  robustness evaluation. The horizontal axis indexes the samples, and the vertical axis denotes the lower bound on the minimum distortion. }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:experiment2}{{5}{9}{Both \ren and $L$ robustness evaluations are derived from the same set of 100 samples. Red points represent the $L$ robustness evaluation, while blue points denote the \ren robustness evaluation. The horizontal axis indexes the samples, and the vertical axis denotes the lower bound on the minimum distortion}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-D}}Comparison among \textsc  {REN}\xspace  , verification, and attack }{9}{subsection.5.4}\protected@file@percent }
\newlabel{exam:comparison}{{\mbox  {V-D}}{9}{Comparison among \ren , verification, and attack}{subsection.5.4}{}}
\citation{yanglyapunov}
\citation{yanglyapunov}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Comparisons on average \textsc  {REN}\xspace  robustness evaluations and distortions fonund by PGD, PGD[\textsc  {REN}\xspace  ] and $\alpha \beta $-CROWN\xspace  . The term '-' means timeout issue. }}{10}{table.caption.7}\protected@file@percent }
\newlabel{table:comparison}{{II}{10}{Comparisons on average \ren robustness evaluations and distortions fonund by PGD, PGD[\ren ] and \abcrown . The term '-' means timeout issue}{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{10}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{VI}{10}{Conclusion}{section.6}{}}
\bibstyle{./IEEEtran}
\bibdata{./IEEEexample}
\bibcite{pacheco2016iot}{1}
\bibcite{naveed2024towards}{2}
\bibcite{morales2024dsl}{3}
\bibcite{chen2023automated}{4}
\bibcite{ayoughi2024enhancing}{5}
\bibcite{weber2024model}{6}
\bibcite{kaufmann2020deep}{7}
\bibcite{madry2017towards}{8}
\bibcite{morgulis2019fooling}{9}
\bibcite{weng2018evaluating}{10}
\bibcite{silva2020opportunities}{11}
\bibcite{amini2021robust}{12}
\bibcite{zhou2022adversarial}{13}
\bibcite{wang2024survey}{14}
\bibcite{bai2021recent}{15}
\bibcite{zhao2024attack}{16}
\bibcite{sathiyanarayanan2018smart}{17}
\bibcite{hassanalian2017classifications}{18}
\bibcite{saunders2024autonomous}{19}
\bibcite{nubert2020safe}{20}
\bibcite{abu2022deep}{21}
\bibcite{kohler2020computationally}{22}
\bibcite{liu2023modeling}{23}
\bibcite{ivanov2021verisig}{24}
\bibcite{huang2022polar}{25}
\bibcite{sha2021synthesizing}{26}
\bibcite{xue2020inner}{27}
\bibcite{xue2021reach}{28}
\bibcite{zhang2023reachability}{29}
\bibcite{dai2021lyapunov}{30}
\bibcite{wu2023neural}{31}
\bibcite{yanglyapunov}{32}
\bibcite{chakraborty2011nonlinear}{33}
\bibcite{chen2014stability}{34}
\bibcite{korda2014controller}{35}
\bibcite{mauroy2016global}{36}
\bibcite{lyapunov1992general}{37}
\bibcite{vidal1988implementing}{38}
\bibcite{luenberger1971introduction}{39}
\@writefile{toc}{\contentsline {section}{References}{11}{section*.8}\protected@file@percent }
\bibcite{meyn2022control}{40}
\bibcite{crary1990techniques}{41}
\bibcite{chen2015disturbance}{42}
\bibcite{wood1996estimation}{43}
\bibcite{ponce2007continuity}{44}
\bibcite{de1963subcompactness}{45}
\bibcite{kechris1990classification}{46}
\bibcite{burr1942cumulative}{47}
\bibcite{smith1990extreme}{48}
\bibcite{snider2009automatic}{49}
\bibcite{mori1976control}{50}
\bibcite{bouabdallah2007full}{51}
\bibcite{brix2024fifth}{52}
\bibcite{du2021fast}{53}
\bibcite{wang2021beta}{54}
\bibcite{massey1951kolmogorov}{55}
\bibcite{jay1981gender}{56}
\gdef \@abspage@last{12}
